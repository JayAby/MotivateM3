from flask import Flask, render_template, request
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
import os

load_dotenv()

app = Flask(__name__)

# Setup Hugging Face client
HF_TOKEN = os.getenv("HF_TOKEN")
print("HF_TOKEN =", HF_TOKEN)
client = InferenceClient(
    model="EleutherAI/gpt-neo-125M", 
    token=HF_TOKEN
)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/generate", methods=["POST"])
def generate():
    feeling = request.form.get("feeling")
    goal = request.form.get("goal")

    # Prompt for the AI
    prompt = f"Give a motivating message to help achieve goal: {goal} considering that they feel: {feeling}."

    # Generate from Hugging Face model
    try:
        response = client.text_generation(
            model="EleutherAI/gpt-neo-125M",
            prompt=prompt,
            max_new_tokens=80
        )
    except Exception as e:
        print("Error:",e)
        message = "Error generating message"
    
    return render_template("result.html", message=message)

if __name__ == "__main__":
    app.run(debug=True)